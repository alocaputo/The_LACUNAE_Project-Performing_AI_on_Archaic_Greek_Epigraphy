{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a5da8d-278b-42a7-b713-dc53c11e87ef",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f168b9",
   "metadata": {},
   "source": [
    "**The dataset full PHI dataset can be obtained from [I.PHI dataset](https://github.com/sommerschield/iphi).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf65bdd6-ef8b-4733-9d5b-e088491eb472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fedbe92-12e0-4949-9adc-f86d14009bf7",
   "metadata": {},
   "source": [
    "## Rebuild the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6bdea6-df35-4c94-820e-ca4d7ea91a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialectal_info = json.load(open('data/iphi-archaic-dialect.json', 'r'))\n",
    "iphi = json.load(open('data/iphi.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30d5dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88695e53-294e-472a-b1c7-9c4e3e4272f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3233/3233 [00:57<00:00, 55.82it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_raw = []\n",
    "for k, v in tqdm(dialectal_info.items()):\n",
    "    current_inscr = None\n",
    "    for inscr in iphi:\n",
    "        if int(k) == inscr['id']:\n",
    "            current_inscr = inscr\n",
    "            break\n",
    "    dataset_raw.append({**current_inscr, 'dialects': v})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d7cc6-a8e6-4a94-94f5-de64112c27d0",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35a145-3998-4fb5-8b7e-531ba22116d4",
   "metadata": {},
   "source": [
    "Loading and splitting the dataset containing the dialectal information.\n",
    "\n",
    "Due to Ithaca's requirments, the inscription $x$ must be:\n",
    "\n",
    " $50 <= |x| <= 750$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79966bcc-ec02-4d3c-a158-f716fa3dd869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset:  3281\n",
      "Filtered:     2351\n",
      "Remaining:     930\n"
     ]
    }
   ],
   "source": [
    "#dataset_raw = json.load(open('data/iphi-archaic_plusDialect_26oct23_fix.json'))\n",
    "\n",
    "dataset_filtered = []\n",
    "for x in dataset_raw:\n",
    "    text = x['text']\n",
    "    text = text.replace('[', '').replace(']', '')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    if 'dialect' in x.keys():\n",
    "        x['dialect'] = x['dialect'].split('.')[2]\n",
    "    elif 'dialect_main_id' in x.keys():\n",
    "        x['dialect'] = x['dialect_main_id'].split('.')[2]\n",
    "        del x['dialect_main_id']\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    text = text.replace(' .', '.')\n",
    "        \n",
    "    if 50 <= len(text) <= 750: # Ithaca requirement\n",
    "        x['text'] = text\n",
    "        dataset_filtered.append(x)\n",
    "    \n",
    "print(\"Raw dataset: \", len(dataset_raw))\n",
    "print(\"Filtered:    \", len(dataset_raw) - len(dataset_filtered))\n",
    "print(\"Remaining:    \", len(dataset_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ba074",
   "metadata": {},
   "source": [
    "Split in Train and Test set following Ithaca splitting rule:\n",
    "\n",
    "```Last digit 3 -> test, 4 -> valid, the rest are the training set``` (see [dataloader.py](https://github.com/google-deepmind/ithaca/blob/ced13193aaa52e49a2388c9ace0244e9a24e6d42/train/dataloader.py#L307))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0577be8-9d18-4177-85a4-d400c8ea7c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 746\n",
      "validation:   89\n",
      "test:   95\n"
     ]
    }
   ],
   "source": [
    "archaic_train_data = [x for x in dataset_filtered if x['id'] % 10 != 3 and x['id'] % 10 != 4]\n",
    "archaic_validation_data = [x for x in dataset_filtered if x['id'] % 10 == 4]\n",
    "archaic_test_data = [x for x in dataset_filtered if x['id'] % 10 == 3]\n",
    "\n",
    "print(f\"train: {len(archaic_train_data)}\\nvalidation:   {len(archaic_validation_data)}\\ntest:   {len(archaic_test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30bbd8ea-0d06-46fa-8f4a-80d4bbfcbe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/archaic\", exist_ok=True)\n",
    "\n",
    "with open('data/archaic/train.json', 'w') as f:\n",
    "        json.dump(archaic_train_data, f)\n",
    "\n",
    "with open('data/archaic/validation.json', 'w') as f:\n",
    "        json.dump(archaic_validation_data, f)\n",
    "\n",
    "with open('data/archaic/test.json', 'w') as f:\n",
    "        json.dump(archaic_test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713b6dd-ecf0-4703-b0c9-ad482a10391e",
   "metadata": {},
   "source": [
    "## Testset masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314536cf-dbea-40b9-a8f4-05aecb9922a7",
   "metadata": {},
   "source": [
    "Using Ithaca masking code for generating mask between 1 and 10 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eff324",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04489622-742b-421c-aa40-158527a56a3a",
   "metadata": {},
   "source": [
    "[text.py](https://github.com/google-deepmind/ithaca/blob/main/ithaca/util/text.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f88e9214-9344-42da-bccc-aaeec1afc702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 the Ithaca Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "def text_to_idx(t, alphabet):\n",
    "  \"\"\"Converts a string to character indices.\"\"\"\n",
    "  return np.array([alphabet.char2idx[c] for c in t], dtype=np.int32)\n",
    "\n",
    "def text_to_word_idx(t, alphabet):\n",
    "  \"\"\"Converts a string to word indices.\"\"\"\n",
    "  out = np.full(len(t), alphabet.word2idx[alphabet.unk], dtype=np.int32)\n",
    "  for m in re.finditer(r'\\w+', t):\n",
    "    if m.group() in alphabet.word2idx:\n",
    "      out[m.start():m.end()] = alphabet.word2idx[m.group()]\n",
    "  return out\n",
    "\n",
    "def random_mask_span(t, geometric_p=0.2, limit_chars=None):\n",
    "  \"\"\"Masks a span of sequential words.\"\"\"\n",
    "\n",
    "  # Obtain span indexes (indlusive)\n",
    "  span_idx = [(ele.start(), ele.end()) for ele in re.finditer(r'[\\w\\s]+', t)]\n",
    "  if not span_idx:\n",
    "    return []\n",
    "\n",
    "  # Select a span to mask\n",
    "  span_start, span_end = random.choice(span_idx)\n",
    "\n",
    "  # Sample a random span length using a geomteric distribution\n",
    "  if geometric_p and limit_chars:\n",
    "    span_len = np.clip(\n",
    "        np.random.geometric(geometric_p),\n",
    "        1, min(limit_chars, span_end - span_start))\n",
    "  elif geometric_p:\n",
    "    span_len = np.clip(\n",
    "        np.random.geometric(geometric_p),\n",
    "        1, span_end - span_start)\n",
    "  elif limit_chars:\n",
    "    span_len = min(limit_chars, span_end - span_start)\n",
    "  else:\n",
    "    raise ValueError('geometric_p or limit_chars should be set.')\n",
    "\n",
    "  # Pick a random start index\n",
    "  span_start = np.random.randint(span_start, span_end - span_len + 1)\n",
    "  assert span_start + span_len <= span_end\n",
    "\n",
    "  # Clip to limit chars\n",
    "  if limit_chars is not None and span_len >= limit_chars:\n",
    "    span_len = limit_chars\n",
    "\n",
    "  # Create mask indices\n",
    "  mask_idx = list(range(span_start, span_start + span_len))\n",
    "\n",
    "  return mask_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b11021b-04de-4979-8c5b-83ca224960ea",
   "metadata": {},
   "source": [
    "[alphabet.py](https://github.com/google-deepmind/ithaca/blob/main/ithaca/util/alphabet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b85295f-75e1-487f-a8d8-d4a1dc3c7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 the Ithaca Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Alphabet classes.\"\"\"\n",
    "\n",
    "class Alphabet:\n",
    "  \"\"\"Generic alphabet class.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               alphabet,\n",
    "               numerals='0',\n",
    "               punctuation='.',\n",
    "               space=' ',\n",
    "               missing='-',\n",
    "               pad='#',\n",
    "               unk='^',\n",
    "               sos='<',\n",
    "               sog='[',\n",
    "               eog=']',\n",
    "               wordlist_file=None,\n",
    "               wordlist_size=100000):\n",
    "    self.alphabet = list(alphabet)  # alph\n",
    "    self.numerals = list(numerals)  # num\n",
    "    self.punctuation = list(punctuation)  # punt\n",
    "    self.space = space  # spacing\n",
    "    self.missing = missing  # missing char\n",
    "    self.pad = pad  # padding (spaces to right of string)\n",
    "    self.unk = unk  # unknown char\n",
    "    self.sos = sos  # start of sentence\n",
    "    self.sog = sog  # start of guess\n",
    "    self.eog = eog  # end of guess\n",
    "\n",
    "    # Define wordlist mapping\n",
    "    idx2word = [self.pad, self.sos, self.unk]\n",
    "    if wordlist_file:\n",
    "      idx2word += [\n",
    "          w_c.split(';')[0]\n",
    "          for w_c in wordlist_file.read().strip().split('\\n')[:wordlist_size]\n",
    "      ]\n",
    "    self.idx2word = np.array(idx2word)\n",
    "    self.word2idx = {self.idx2word[i]: i for i in range(len(self.idx2word))}\n",
    "\n",
    "    # Define vocab mapping\n",
    "    self.idx2char = np.array(\n",
    "        [self.pad, self.sos, self.unk, self.space, self.missing] +\n",
    "        self.alphabet + self.numerals + self.punctuation)\n",
    "    self.char2idx = {self.idx2char[i]: i for i in range(len(self.idx2char))}\n",
    "\n",
    "    # Define special character indices\n",
    "    self.pad_idx = self.char2idx[pad]\n",
    "    self.sos_idx = self.char2idx[sos]\n",
    "    self.unk_idx = self.char2idx[unk]\n",
    "    self.alphabet_start_idx = self.char2idx[self.alphabet[0]]\n",
    "    self.alphabet_end_idx = self.char2idx[self.numerals[-1]]\n",
    "\n",
    "  def filter(self, t):\n",
    "    return t\n",
    "\n",
    "  def size_char(self):\n",
    "    return len(self.idx2char)\n",
    "\n",
    "  def size_word(self):\n",
    "    return len(self.idx2word)\n",
    "\n",
    "\n",
    "class GreekAlphabet(Alphabet):\n",
    "  \"\"\"Greek alphabet class.\"\"\"\n",
    "\n",
    "  def __init__(self, wordlist_file=None, wordlist_size=100000):\n",
    "    greek_alphabet = 'αβγδεζηθικλμνξοπρςστυφχψωϙϛ'\n",
    "\n",
    "    super().__init__(\n",
    "        alphabet=greek_alphabet,\n",
    "        wordlist_file=wordlist_file,\n",
    "        wordlist_size=wordlist_size)\n",
    "    self.tonos_to_oxia = {\n",
    "        # tonos  : #oxia\n",
    "        u'\\u0386': u'\\u1FBB',  # capital letter alpha\n",
    "        u'\\u0388': u'\\u1FC9',  # capital letter epsilon\n",
    "        u'\\u0389': u'\\u1FCB',  # capital letter eta\n",
    "        u'\\u038C': u'\\u1FF9',  # capital letter omicron\n",
    "        u'\\u038A': u'\\u1FDB',  # capital letter iota\n",
    "        u'\\u038E': u'\\u1FF9',  # capital letter upsilon\n",
    "        u'\\u038F': u'\\u1FFB',  # capital letter omega\n",
    "        u'\\u03AC': u'\\u1F71',  # small letter alpha\n",
    "        u'\\u03AD': u'\\u1F73',  # small letter epsilon\n",
    "        u'\\u03AE': u'\\u1F75',  # small letter eta\n",
    "        u'\\u0390': u'\\u1FD3',  # small letter iota with dialytika and tonos/oxia\n",
    "        u'\\u03AF': u'\\u1F77',  # small letter iota\n",
    "        u'\\u03CC': u'\\u1F79',  # small letter omicron\n",
    "        u'\\u03B0': u'\\u1FE3',\n",
    "        # small letter upsilon with dialytika and tonos/oxia\n",
    "        u'\\u03CD': u'\\u1F7B',  # small letter upsilon\n",
    "        u'\\u03CE': u'\\u1F7D'  # small letter omega\n",
    "    }\n",
    "    self.oxia_to_tonos = {v: k for k, v in self.tonos_to_oxia.items()}\n",
    "\n",
    "  def filter(self, t):  # override previous filter function\n",
    "    # lowercase\n",
    "    t = t.lower()\n",
    "\n",
    "    # replace dot below\n",
    "    t = t.replace(u'\\u0323', '')\n",
    "\n",
    "    # replace perispomeni\n",
    "    t = t.replace(u'\\u0342', '')\n",
    "    t = t.replace(u'\\u02C9', '')\n",
    "\n",
    "    # replace ending sigma\n",
    "    t = re.sub(r'([\\w\\[\\]])σ(?![\\[\\]])(\\b)', r'\\1ς\\2', t)\n",
    "\n",
    "    # replace oxia with tonos\n",
    "    for oxia, tonos in self.oxia_to_tonos.items():\n",
    "      t = t.replace(oxia, tonos)\n",
    "\n",
    "    # replace h\n",
    "    h_patterns = {\n",
    "        # input: #target\n",
    "        'ε': 'ἑ',\n",
    "        'ὲ': 'ἓ',\n",
    "        'έ': 'ἕ',\n",
    "        'α': 'ἁ',\n",
    "        'ὰ': 'ἃ',\n",
    "        'ά': 'ἅ',\n",
    "        'ᾶ': 'ἇ',\n",
    "        'ι': 'ἱ',\n",
    "        'ὶ': 'ἳ',\n",
    "        'ί': 'ἵ',\n",
    "        'ῖ': 'ἷ',\n",
    "        'ο': 'ὁ',\n",
    "        'ό': 'ὅ',\n",
    "        'ὸ': 'ὃ',\n",
    "        'υ': 'ὑ',\n",
    "        'ὺ': 'ὓ',\n",
    "        'ύ': 'ὕ',\n",
    "        'ῦ': 'ὗ',\n",
    "        'ὴ': 'ἣ',\n",
    "        'η': 'ἡ',\n",
    "        'ή': 'ἥ',\n",
    "        'ῆ': 'ἧ',\n",
    "        'ὼ': 'ὣ',\n",
    "        'ώ': 'ὥ',\n",
    "        'ω': 'ὡ',\n",
    "        'ῶ': 'ὧ'\n",
    "    }\n",
    "\n",
    "    # iterate by keys\n",
    "    for h_in, h_tar in h_patterns.items():\n",
    "      # look up and replace h[ and h]\n",
    "      t = re.sub(r'ℎ(\\[?){}'.format(h_in), r'\\1{}'.format(h_tar), t)\n",
    "      t = re.sub(r'ℎ(\\]?){}'.format(h_in), r'{}\\1'.format(h_tar), t)\n",
    "\n",
    "    # any h left is an ἡ\n",
    "    t = re.sub(r'(\\[?)ℎ(\\]?)', r'\\1ἡ\\2', t)\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87ebc706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer start for prepending start of sentence character\n",
    "def mask_text(text, alphabet):\n",
    "  prepend_sos = 1\n",
    "  char_mask_rate_min = 0.\n",
    "  char_mask_rate_max = 0.5\n",
    "  span_mask_geometric_p=0.1\n",
    "  span_mask_eval_len=10\n",
    "  span_mask_ratio=0.15\n",
    "  mode = 'valid'\n",
    "\n",
    "  start_sample_idx = int(prepend_sos)\n",
    "\n",
    "  # Mask text\n",
    "  text_mask = np.zeros(len(text), dtype=bool)\n",
    "  if mode in ['train', 'valid']:\n",
    "    text_list = list(text)\n",
    "\n",
    "    # Non missing idx (avoid removing start of sentence character)\n",
    "    non_missing_idx = []\n",
    "    for i in range(start_sample_idx, len(text_list)):\n",
    "      if text_list[i] not in [alphabet.missing] + alphabet.punctuation:\n",
    "        non_missing_idx.append(i)\n",
    "\n",
    "    # Skip sample if there are no usable characters\n",
    "    if not non_missing_idx:\n",
    "      print(\"Something is wrong\")\n",
    "\n",
    "    char_mask_idx = []\n",
    "    if char_mask_rate_max > 0.:\n",
    "      # Compute rate\n",
    "      char_mask_rate = np.random.uniform(char_mask_rate_min,\n",
    "                                          char_mask_rate_max)\n",
    "\n",
    "      # Fix masking in valid mode for comparing experiments\n",
    "      span_mask_geometric_p = span_mask_geometric_p\n",
    "      mask_num_total = int(char_mask_rate * len(non_missing_idx))\n",
    "      mask_num_span = int(mask_num_total * span_mask_ratio)\n",
    "      \n",
    "      if mode == 'valid' and span_mask_eval_len > 0:\n",
    "        span_mask_geometric_p = None\n",
    "        mask_num_total = min(span_mask_eval_len, len(non_missing_idx))\n",
    "        mask_num_span = mask_num_total\n",
    "      mask_num_char = mask_num_total - mask_num_span\n",
    "\n",
    "      # Mask random indices\n",
    "      if mask_num_char > 0:\n",
    "        char_mask_idx = np.random.choice(\n",
    "            non_missing_idx, mask_num_char, replace=False).tolist()\n",
    "\n",
    "      # Mask random spans\n",
    "      if mask_num_span > 0:\n",
    "        count_span = 0\n",
    "        span_mask_idx = []\n",
    "        while (len(span_mask_idx) < mask_num_span and count_span < 1):\n",
    "          span_mask_idx.extend(\n",
    "              random_mask_span(\n",
    "                  text,\n",
    "                  geometric_p=span_mask_geometric_p,\n",
    "                  limit_chars=random.randint(1,10)))\n",
    "          count_span += 1\n",
    "        char_mask_idx.extend(span_mask_idx)\n",
    "\n",
    "    # Mask text\n",
    "    for idx in set(char_mask_idx):\n",
    "      text_mask[idx] = True\n",
    "      text_list[idx] = '?'\n",
    "    text = ''.join(text_list)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2992e52",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa06eb48-fd75-4779-8a6f-c83800292418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 13256.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# random.seed(42)\n",
    "\n",
    "# test_data_masked = []\n",
    "# alphabet = GreekAlphabet()\n",
    "\n",
    "# for inscription in tqdm(archaic_test_data):\n",
    "#     text = inscription['text']\n",
    "#     inscription['masked'] = mask_text(text, alphabet)\n",
    "#     test_data_masked.append(inscription)\n",
    "            \n",
    "# json.dump(test_data_masked, open('data/archaic/test_masked.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586a82f",
   "metadata": {},
   "source": [
    "## Preparing the (archaic) knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18fe6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/archaic/train.txt', 'w') as f:\n",
    "    for inscription in archaic_train_data:\n",
    "        f.write(f\"{inscription['text']}\\n\")\n",
    "\n",
    "with open('data/archaic/validation.txt', 'w') as f:\n",
    "    for inscription in archaic_validation_data:\n",
    "        f.write(f\"{inscription['text']}\\n\")\n",
    "\n",
    "with open('data/archaic/test.txt', 'w') as f:\n",
    "    for inscription in archaic_test_data:\n",
    "        f.write(f\"{inscription['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8015dc34",
   "metadata": {},
   "source": [
    "## Full i.PHI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f146e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tmp = {int(d['id']): d for d in json.load(open('data/iphi.json'))}\n",
    "assert len(dataset_tmp) == 178551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8d03fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9441\n"
     ]
    }
   ],
   "source": [
    "rev_dataset = {}\n",
    "\n",
    "black_list=set([\n",
    "                      # 2334, 10, 293931, 14, 293752, 15, 293753, 16, 11,\n",
    "                      # 294468, 229647, 12, 291324, 291317, 17, 232697, 293754,\n",
    "                      # 1682, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 291118,\n",
    "                      # 291320, 291319, 292366, 34, 291960, 35, 32, 346490, 27,\n",
    "                      # 292187, 291318, 19, 18, 37, 291321, 292189, 293756, 42,\n",
    "                      # 46, 232710, 39, 40, 41, 291322, 293757, 293327, 28,\n",
    "                      # 292194, 293326, 21, 293755, 291319, 291117, 38, 291959,\n",
    "                      # 31, 232705\n",
    "                  ])\n",
    "\n",
    "if black_list:\n",
    "    print(black_list)\n",
    "\n",
    "for key in sorted(dataset_tmp.keys()):\n",
    "    value = dataset_tmp[key]\n",
    "    rev_dataset.setdefault(value['text'], set()).add(key)\n",
    "    if len(rev_dataset[value['text']]) > 1:\n",
    "        black_list.add(int(value['id']))\n",
    "del rev_dataset\n",
    "print(len(black_list))\n",
    "\n",
    "#assert len(black_list) == 9441\n",
    "\n",
    "# Create deduplicated dataset\n",
    "dataset = []\n",
    "for d in dataset_tmp.values():\n",
    "    if int(d['id']) not in black_list:\n",
    "        dataset.append(d)\n",
    "del dataset_tmp\n",
    "del black_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a7f662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_region_maps(region_file):\n",
    "  \"\"\"Extracts creates a map from PHI region id to a continuous region id.\"\"\"\n",
    "  region_ids = []  # Used mainly for eval\n",
    "  region_ids_inv = {}  # Used in data loader\n",
    "  region_names_inv = {}  # Used in eval\n",
    "  for l in region_file.read().strip().split('\\n'):\n",
    "    tok_name_id, _ = l.strip().split(';')  # second field is frequency, unused\n",
    "    region_name, region_id = tok_name_id.split('_')\n",
    "    region_name = region_name.strip()\n",
    "    region_id = int(region_id)\n",
    "    # Ignore unknown regions:\n",
    "    if ((region_name == 'Unknown Provenances' and region_id == 884) or\n",
    "        (region_name == 'unspecified subregion' and region_id == 885) or\n",
    "        (region_name == 'unspecified subregion' and region_id == 1439)):\n",
    "      continue\n",
    "    region_ids.append(region_id)\n",
    "    region_ids_inv[region_id] = len(region_ids_inv)\n",
    "    region_names_inv[len(region_names_inv)] = region_name\n",
    "\n",
    "  return {\n",
    "      'ids': region_ids,\n",
    "      'ids_inv': region_ids_inv,\n",
    "      'names_inv': region_names_inv\n",
    "  }\n",
    "\n",
    "region_map = dict()\n",
    "region_map['main'] = load_region_maps(open('data/iphi-region-main.txt', 'r'))\n",
    "region_map['sub'] = load_region_maps(open('data/iphi-region-sub.txt', 'r'))\n",
    "alphabet = GreekAlphabet(wordlist_file=open('data/iphi-wordlist.txt', 'r'), wordlist_size=35884)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42803b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78591\n"
     ]
    }
   ],
   "source": [
    "final_data = []\n",
    "char_use_guess=True\n",
    "context_char_min=50\n",
    "\n",
    "for sample in dataset:\n",
    "\n",
    "  new_sample = sample.copy()\n",
    "\n",
    "  # Skip if region does not exist in map\n",
    "  if (int(sample['region_main_id']) not in region_map['main']['ids_inv'] or\n",
    "    int(sample['region_sub_id']) not in region_map['sub']['ids_inv']):\n",
    "    continue\n",
    "\n",
    "  # Replace guess signs with missing chars\n",
    "  if not char_use_guess:\n",
    "    sample['text'] = re.sub(r'\\[(.*?)\\]', lambda m: '-' * len(m.group(1)),\n",
    "                            sample['text'])\n",
    "  sample['text'] = sample['text'].replace(alphabet.sog,\n",
    "                                          '').replace(alphabet.eog, '')\n",
    "  \n",
    "  sample['text'].replace('[', '').replace(']', '')\n",
    "  sample['text'] = sample['text'].strip()\n",
    "  sample['text'] = re.sub(r'\\s+', ' ', sample['text'])\n",
    "\n",
    "  # Filter by text length\n",
    "  if len(sample['text'].replace(alphabet.missing,\n",
    "                                '')) < context_char_min:\n",
    "    continue\n",
    "  \n",
    "  sample['text'] = sample['text'].replace(' .', '.')\n",
    "    \n",
    "  if '  ' in sample['text']:\n",
    "    print(sample['text'])\n",
    "  final_data.append(sample)\n",
    "print(len(final_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04ce5b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 63002\n",
      "validation:   7780\n",
      "test:   7809\n"
     ]
    }
   ],
   "source": [
    "iphi_train_data = [x for x in final_data if x['id'] % 10 != 3 and x['id'] % 10 != 4]\n",
    "iphi_validation_data = [x for x in final_data if x['id'] % 10 == 4]\n",
    "iphi_test_data = [x for x in final_data if x['id'] % 10 == 3]\n",
    "\n",
    "print(f\"train: {len(iphi_train_data)}\\nvalidation:   {len(iphi_validation_data)}\\ntest:   {len(iphi_test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d6f2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/iphi\", exist_ok=True)\n",
    "\n",
    "with open('data/iphi/train.json', 'w') as f:\n",
    "    json.dump(iphi_train_data, f)\n",
    "\n",
    "with open('data/iphi/validation.json', 'w') as f:\n",
    "        json.dump(iphi_validation_data, f)\n",
    "\n",
    "with open('data/iphi/test.json', 'w') as f:\n",
    "        json.dump(iphi_test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "771b6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/iphi/train.txt', 'w') as f:\n",
    "    for inscription in iphi_train_data:\n",
    "        f.write(f\"{inscription['text']}\\n\")\n",
    "\n",
    "with open('data/iphi/validation.txt', 'w') as f:\n",
    "    for inscription in iphi_validation_data:\n",
    "        f.write(f\"{inscription['text']}\\n\")\n",
    "\n",
    "with open('data/iphi/test.txt', 'w') as f:\n",
    "    for inscription in iphi_test_data:\n",
    "        f.write(f\"{inscription['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fef3d8",
   "metadata": {},
   "source": [
    "# Same masking for agBERT and Ithaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e4cc3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 157743.02it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/archaic/test.txt') as f:\n",
    "   test = f.readlines()\n",
    "\n",
    "\n",
    "# test_json = json.load(open('data/archaic/test_masked.json'))\n",
    "test_json = json.load(open('data/archaic/test.json'))\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "common_gaps = []\n",
    "\n",
    "for inscription in tqdm(test_json):\n",
    "\n",
    "    sentence = inscription['text']\n",
    "    sentence_split = sentence.split()\n",
    "    sentence_ithaca = sentence_split.copy()\n",
    "\n",
    "    rand_int = random.randint(0, len(sentence_split)-1)\n",
    "    ground_truth = sentence_split[rand_int]\n",
    "    while '-' in ground_truth:\n",
    "        rand_int = random.randint(0, len(sentence_split)-1)\n",
    "        ground_truth = sentence_split[rand_int]\n",
    "    if '.' in ground_truth:\n",
    "        sentence_split[rand_int] = '[MASK].'\n",
    "        sentence_ithaca[rand_int] = '?'*(len(ground_truth)-1) + '.'\n",
    "        ground_truth = ground_truth.replace('.', '')\n",
    "    else:\n",
    "        sentence_split[rand_int] = '[MASK]'\n",
    "        sentence_ithaca[rand_int] = '?'*len(ground_truth)\n",
    "\n",
    "    masked = ' '.join(sentence_split)\n",
    "    sentence_ithaca = ' '.join(sentence_ithaca)\n",
    "\n",
    "    inscription['masked_ag'] = masked\n",
    "\n",
    "    inscription['masked_ithaca'] = sentence_ithaca\n",
    "\n",
    "    inscription['masked_gt'] = ground_truth\n",
    "    \n",
    "    common_gaps.append(inscription)\n",
    "\n",
    "with open('data/archaic/test_common.json', 'w') as f:\n",
    "    json.dump(common_gaps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31f093f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:00<00:00, 70406.08it/s]\n"
     ]
    }
   ],
   "source": [
    "archaic_validation_data = json.load(open('data/archaic/validation.json'))\n",
    "random.seed(0)\n",
    "\n",
    "common_gaps = []\n",
    "\n",
    "for inscription in tqdm(archaic_validation_data):\n",
    "    tolerance = 1000    \n",
    "    sentence = inscription['text']\n",
    "    sentence_split = sentence.split()\n",
    "    sentence_ithaca = sentence_split.copy()\n",
    "\n",
    "    rand_int = random.randint(0, len(sentence_split)-1)\n",
    "    ground_truth = sentence_split[rand_int]\n",
    "    while '-' in ground_truth:\n",
    "        rand_int = random.randint(0, len(sentence_split)-1)\n",
    "        ground_truth = sentence_split[rand_int]\n",
    "        tolerance -= 1\n",
    "        if tolerance == 0:\n",
    "            break\n",
    "    if tolerance == 0:\n",
    "        continue\n",
    "    if '.' in ground_truth:\n",
    "        sentence_split[rand_int] = '[MASK].'\n",
    "        sentence_ithaca[rand_int] = '?'*(len(ground_truth)-1) + '.'\n",
    "        ground_truth = ground_truth.replace('.', '')\n",
    "    else:\n",
    "        sentence_split[rand_int] = '[MASK]'\n",
    "        sentence_ithaca[rand_int] = '?'*len(ground_truth)\n",
    "\n",
    "    masked = ' '.join(sentence_split)\n",
    "    sentence_ithaca = ' '.join(sentence_ithaca)\n",
    "\n",
    "    inscription['masked_ag'] = masked\n",
    "\n",
    "    inscription['masked_ithaca'] = sentence_ithaca\n",
    "\n",
    "    inscription['masked_gt'] = ground_truth\n",
    "    \n",
    "    common_gaps.append(inscription)\n",
    "\n",
    "with open('data/archaic/validation_common.json', 'w') as f:\n",
    "    json.dump(common_gaps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cc4798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in common_gaps:\n",
    "    if '-' in x['masked_gt']:\n",
    "        print(x['text'])\n",
    "        print(x['masked_ag'])\n",
    "        print(x['masked_ithaca'])\n",
    "        print(x['masked_gt'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0dedeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aa in common_gaps:\n",
    "    if '.' in aa['masked_gt']:\n",
    "        print(aa['text'])\n",
    "        print(aa['masked_ithaca'])\n",
    "        print(aa['masked_ag'])\n",
    "        print(aa['masked_gt'])\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ea9d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in common_gaps:\n",
    "    if x['id'] == 290213:\n",
    "        print(x['text'])\n",
    "        print(x['masked_ithaca'])\n",
    "    if x['masked_ithaca'][-1] == '?':\n",
    "        print(x['text'])\n",
    "        print(x['masked'])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5782eb1",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "599ad9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:00<00:00, 212011.11it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/archaic/iphi_archaic_train.txt') as f:\n",
    "   test = f.readlines()\n",
    "\n",
    "\n",
    "test_json = json.load(open('data/archaic/iphi_archaic_train.json'))\n",
    "random.seed(0)\n",
    "\n",
    "rag_gaps = []\n",
    "\n",
    "os.makedirs(\"data/rag\", exist_ok=True)\n",
    "\n",
    "for inscription in tqdm(test_json):\n",
    "\n",
    "    sentence = inscription['text']\n",
    "    sentence = sentence.strip()\n",
    "    sentence_split = sentence.split()\n",
    "    sentence_ithaca = sentence_split.copy()\n",
    "\n",
    "    rand_int = random.randint(0, len(sentence_split)-1)\n",
    "    ground_truth = sentence_split[rand_int]\n",
    "\n",
    "    sentence_split[rand_int] = '[MASK]'\n",
    "    \n",
    "    if '.' in ground_truth:\n",
    "        sentence_split[rand_int] = '[MASK].'\n",
    "        sentence_ithaca[rand_int] = '?'*(len(ground_truth)-1) + '.'\n",
    "        ground_truth = ground_truth.replace('.', '')\n",
    "    else:\n",
    "        sentence_split[rand_int] = '[MASK]'\n",
    "        sentence_ithaca[rand_int] = '?'*len(ground_truth)\n",
    "\n",
    "    masked = ' '.join(sentence_split)\n",
    "    sentence_ithaca = ' '.join(sentence_ithaca)\n",
    "\n",
    "    inscription['masked_ag'] = masked\n",
    "\n",
    "    inscription['masked_ithaca'] = sentence_ithaca\n",
    "\n",
    "    inscription['masked_gt'] = ground_truth\n",
    "    \n",
    "    rag_gaps.append(inscription)\n",
    "\n",
    "with open('data/rag/iphi_train.json', 'w') as f:\n",
    "    json.dump(rag_gaps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abc37170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7809/7809 [00:00<00:00, 126173.76it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/iphi/test.txt') as f:\n",
    "   test = f.readlines()\n",
    "\n",
    "\n",
    "test_json = json.load(open('data/iphi/test.json'))\n",
    "random.seed(0)\n",
    "\n",
    "rag_gaps = []\n",
    "\n",
    "os.makedirs(\"data/rag\", exist_ok=True)\n",
    "\n",
    "for inscription in tqdm(test_json):\n",
    "\n",
    "    sentence = inscription['text']\n",
    "    sentence = sentence.strip()\n",
    "    sentence_split = sentence.split()\n",
    "    sentence_ithaca = sentence_split.copy()\n",
    "\n",
    "    rand_int = random.randint(0, len(sentence_split)-1)\n",
    "    ground_truth = sentence_split[rand_int]\n",
    "\n",
    "    sentence_split[rand_int] = '[MASK]'\n",
    "    \n",
    "    if '.' in ground_truth:\n",
    "        sentence_split[rand_int] = '[MASK].'\n",
    "        sentence_ithaca[rand_int] = '?'*(len(ground_truth)-1) + '.'\n",
    "        ground_truth = ground_truth.replace('.', '')\n",
    "    else:\n",
    "        sentence_split[rand_int] = '[MASK]'\n",
    "        sentence_ithaca[rand_int] = '?'*len(ground_truth)\n",
    "\n",
    "    masked = ' '.join(sentence_split)\n",
    "    sentence_ithaca = ' '.join(sentence_ithaca)\n",
    "\n",
    "    inscription['masked_ag'] = masked\n",
    "\n",
    "    inscription['masked_ithaca'] = sentence_ithaca\n",
    "\n",
    "    inscription['masked_gt'] = ground_truth\n",
    "    \n",
    "    rag_gaps.append(inscription)\n",
    "\n",
    "with open('data/rag/iphi_test_rag.json', 'w') as f:\n",
    "    json.dump(rag_gaps, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804d8c3",
   "metadata": {},
   "source": [
    "# Training Archaic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b49884",
   "metadata": {},
   "source": [
    "750-450 BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95d57fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iphi_train = json.load(open('data/iphi/train.json'))\n",
    "iphi_validation = json.load(open('data/iphi/validation.json'))\n",
    "iphi_test = json.load(open('data/iphi/test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90b96aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iphi_archaic_train = []\n",
    "for inscription in iphi_train:\n",
    "    if inscription['date_min'] == None or inscription['date_max'] == None:\n",
    "        continue\n",
    "    if inscription['date_min'] == None or inscription['date_max'] == None:\n",
    "        continue\n",
    "    date_min = int(inscription['date_min'])\n",
    "    date_max = int(inscription['date_max'])\n",
    "    if not 50 <= len(inscription['text']) <= 750:\n",
    "        continue\n",
    "    if -750 <= date_min <= -450 and -750 <= date_max <= -450:\n",
    "        iphi_archaic_train.append(inscription)\n",
    "\n",
    "iphi_archaic_validation = []\n",
    "for inscription in iphi_validation:\n",
    "    if inscription['date_min'] == None or inscription['date_max'] == None:\n",
    "        continue\n",
    "    if inscription['date_min'] == None or inscription['date_max'] == None:\n",
    "        continue\n",
    "    date_min = int(inscription['date_min'])\n",
    "    date_max = int(inscription['date_max'])\n",
    "    if not 50 <= len(inscription['text']) <= 750:\n",
    "        continue\n",
    "    if -750 <= date_min <= -450 and -750 <= date_max <= -450:\n",
    "        iphi_archaic_validation.append(inscription)\n",
    "\n",
    "iphi_archaic_test = []\n",
    "for inscription in iphi_test:\n",
    "    if inscription['date_min'] == None or inscription['date_max'] == None:\n",
    "        continue\n",
    "    if inscription['date_min'] == None or inscription['date_max'] == None:\n",
    "        continue\n",
    "    date_min = int(inscription['date_min'])\n",
    "    date_max = int(inscription['date_max'])\n",
    "    if not 50 <= len(inscription['text']) <= 750:\n",
    "        continue\n",
    "    if -750 <= date_min <= -450 and -750 <= date_max <= -450:\n",
    "        iphi_archaic_test.append(inscription)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54745bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/archaic\", exist_ok=True)\n",
    "\n",
    "with open('data/archaic/iphi_archaic_train.json', 'w') as f:\n",
    "    json.dump(iphi_archaic_train, f)\n",
    "\n",
    "with open('data/archaic/iphi_archaic_validation.json', 'w') as f:\n",
    "    json.dump(iphi_archaic_test, f)\n",
    "\n",
    "with open('data/archaic/iphi_archaic_test.json', 'w') as f:\n",
    "    json.dump(iphi_archaic_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc10711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/archaic/iphi_archaic_train.txt', 'w') as f:\n",
    "    for inscription in iphi_archaic_train:\n",
    "        f.write(f\"{inscription['text']}\\n\")\n",
    "\n",
    "with open('data/archaic/iphi_archaic_validation.txt', 'w') as f:\n",
    "    for inscription in iphi_archaic_test:\n",
    "        f.write(f\"{inscription['text']}\\n\")\n",
    "\n",
    "with open('data/archaic/iphi_archaic_test.txt', 'w') as f:\n",
    "    for inscription in iphi_archaic_test:\n",
    "        f.write(f\"{inscription['text']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
