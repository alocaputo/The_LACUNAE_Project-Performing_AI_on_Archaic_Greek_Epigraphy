{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import editdistance\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, BertForMaskedLM, BertTokenizer, BertConfig\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = json.load(open('data/archaic/test_common.json'))\n",
    "max_l = max([ len(x['masked_gt']) for x in test_set])\n",
    "\n",
    "dialects = ['atticionic', 'doric', 'northwest', 'aeolic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 the Ithaca Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Example for running inference. See also colab.\"\"\"\n",
    "\n",
    "import functools\n",
    "import pickle\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from ithaca.eval import inference\n",
    "from ithaca.models.model import Model\n",
    "from ithaca.util.alphabet import GreekAlphabet\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path):\n",
    "  \"\"\"Loads a checkpoint pickle.\n",
    "\n",
    "  Args:\n",
    "    path: path to checkpoint pickle\n",
    "\n",
    "  Returns:\n",
    "    a model config dictionary (arguments to the model's constructor), a dict of\n",
    "    dicts containing region mapping information, a GreekAlphabet instance with\n",
    "    indices and words populated from the checkpoint, a dict of Jax arrays\n",
    "    `params`, and a `forward` function.\n",
    "  \"\"\"\n",
    "\n",
    "  # Pickled checkpoint dict containing params and various config:\n",
    "  with open(path, 'rb') as f:\n",
    "    checkpoint = pickle.load(f)\n",
    "\n",
    "  # We reconstruct the model using the same arguments as during training, which\n",
    "  # are saved as a dict in the \"model_config\" key, and construct a `forward`\n",
    "  # function of the form required by attribute() and restore().\n",
    "  params = jax.device_put(checkpoint['params'])\n",
    "  model = Model(**checkpoint['model_config'])\n",
    "  forward = functools.partial(model.apply, params)\n",
    "\n",
    "  # Contains the mapping between region IDs and names:\n",
    "  region_map = checkpoint['region_map']\n",
    "\n",
    "  # Use vocabulary mapping from the checkpoint, the rest of the values in the\n",
    "  # class are fixed and constant e.g. the padding symbol\n",
    "  alphabet = GreekAlphabet()\n",
    "  alphabet.idx2word = checkpoint['alphabet']['idx2word']\n",
    "  alphabet.word2idx = checkpoint['alphabet']['word2idx']\n",
    "\n",
    "  return checkpoint['model_config'], region_map, alphabet, params, forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ithaca base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/pty.py:85: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 95/95 [01:24<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 run_ithaca_inference.py --results_path results/archaic/ithaca --cuda 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== atticionic ========\n",
      "ACC1 : 28.15%\n",
      "ACC20: 45.18%\n",
      "CER  : 65.16%\n",
      "======== doric ========\n",
      "ACC1 : 25.00%\n",
      "ACC20: 56.67%\n",
      "CER  : 60.45%\n",
      "======== northwest ========\n",
      "ACC1 : 40.00%\n",
      "ACC20: 70.00%\n",
      "CER  : 50.83%\n",
      "======== aeolic ========\n",
      "ACC1 : 33.33%\n",
      "ACC20: 33.33%\n",
      "CER  : 66.67%\n",
      "======== TOTAL ========\n",
      "CER : 67.11%\n",
      "ACC1 : 24.24%\n",
      "ACC20 : 42.66%\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('results/archaic/ithaca', exist_ok=True)\n",
    "\n",
    "editd = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "acc1 = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "acc20 = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "\n",
    "for inscription in test_set:\n",
    "\n",
    "    ground_truth = inscription['text'].strip()\n",
    "    text_masked = inscription['masked_ithaca']\n",
    "    phi_id = inscription['id']\n",
    "\n",
    "    dialect = inscription['dialect']\n",
    "\n",
    "    predictions = json.load(open(f'results/archaic/ithaca/{phi_id}.json'))['predictions']\n",
    "\n",
    "    text_predictions = [ pred['text'] for pred in predictions]\n",
    "\n",
    "    assert len(text_predictions[0]) == len(ground_truth)\n",
    "    ctop1 = 0\n",
    "    ctop20 = 0\n",
    "    if ground_truth in text_predictions[:1]:\n",
    "        ctop1 = 1\n",
    "        ctop20 = 1\n",
    "    if ground_truth in text_predictions:\n",
    "        ctop20 = 1\n",
    "\n",
    "    l = text_masked.count('?')\n",
    "\n",
    "    acc1[dialect][l].append(ctop1)\n",
    "    acc20[dialect][l].append(ctop20)\n",
    "\n",
    "    editd[dialect][l].append(editdistance.eval(text_predictions[0], ground_truth) / l)\n",
    "\n",
    "for dialect in dialects:\n",
    "    print(f\"======== {dialect} ========\")\n",
    "    print(f'ACC1 : {np.mean([np.mean(v) for v in acc1[dialect].values() if v])*100:.2f}%')\n",
    "    print(f'ACC20: {np.mean([np.mean(v) for v in acc20[dialect].values() if v])*100:.2f}%')\n",
    "    print(f'CER  : {np.mean([np.mean(v) for v in editd[dialect].values() if v])*100:.2f}%')\n",
    "\n",
    "print(f\"======== TOTAL ========\")\n",
    "cert = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in editd.items():\n",
    "    for kk, v in vs.items():\n",
    "        cert[kk].extend(v)\n",
    "acc1t = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in acc1.items():\n",
    "    for kk, v in vs.items():\n",
    "        acc1t[kk].extend(v)\n",
    "\n",
    "acc20t = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in acc20.items():\n",
    "    for kk, v in vs.items():\n",
    "        acc20t[kk].extend(v)\n",
    "print(f'CER : {np.mean([np.mean(v) for v in cert.values() if v])*100:.2f}%')\n",
    "print(f'ACC1 : {np.mean([np.mean(v) for v in acc1t.values() if v])*100:.2f}%')\n",
    "print(f'ACC20 : {np.mean([np.mean(v) for v in acc20t.values() if v])*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AG BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at pranaydeeps/Ancient-Greek-BERT and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 95/95 [00:33<00:00,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== atticionic ========\n",
      "CER  : 93.19%\n",
      "ACC1 : 0.00%\n",
      "ACC20: 0.00%\n",
      "======== doric ========\n",
      "CER  : 96.10%\n",
      "ACC1 : 0.00%\n",
      "ACC20: 0.00%\n",
      "======== northwest ========\n",
      "CER  : 95.83%\n",
      "ACC1 : 0.00%\n",
      "ACC20: 0.00%\n",
      "======== aeolic ========\n",
      "CER  : 79.17%\n",
      "ACC1 : 0.00%\n",
      "ACC20: 0.00%\n",
      "======== TOTAL ========\n",
      "CER : 91.45%\n",
      "ACC1 : 0.00%\n",
      "ACC20 : 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"pranaydeeps/Ancient-Greek-BERT\"\n",
    "fill_mask = pipeline(\"fill-mask\", model=model_name, tokenizer=model_name, top_k=3500)\n",
    "\n",
    "ctop1s = []\n",
    "ctop20s = []\n",
    "#editd = { x: [] for x in range(0,20)}\n",
    "#acc1 =  { x: [] for x in range(1, max_l+1)}\n",
    "#acc20 = { x: [] for x in range(1, max_l+1)}\n",
    "editd = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "acc1 = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "acc20 = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "for sentence in tqdm(test_set):\n",
    "\n",
    "    masked = sentence['masked_ag']\n",
    "    ground_truth = sentence['masked_gt']\n",
    "\n",
    "    dialect = sentence['dialect']\n",
    "\n",
    "    inference_result = fill_mask(masked)\n",
    "\n",
    "    constr = [ x for x in inference_result if len(x['token_str']) == len(ground_truth)][:20]\n",
    "\n",
    "    ctop1 = 0\n",
    "    ctop20 = 0\n",
    "    if ground_truth in [ x['token_str'] for x in constr[:1] ]:\n",
    "        ctop20 = 1\n",
    "        ctop1 = 1\n",
    "    if ground_truth in [ x['token_str'] for x in constr ]:\n",
    "        ctop20 = 1\n",
    "\n",
    "    ctop1s.append(ctop1)\n",
    "    ctop20s.append(ctop20)\n",
    "    #editd[len(ground_truth)].append(editdistance.eval(constr[0]['token_str'], ground_truth) / len(ground_truth))\n",
    "    #acc1[len(ground_truth)].append(ctop1)\n",
    "    #acc20[len(ground_truth)].append(ctop20)\n",
    "    acc1[dialect][len(ground_truth)].append(ctop1)\n",
    "    acc20[dialect][len(ground_truth)].append(ctop20)\n",
    "\n",
    "    if constr != []:\n",
    "        editd[dialect][len(ground_truth)].append(editdistance.eval(constr[0]['token_str'], ground_truth) / len(ground_truth))\n",
    "    else:\n",
    "        editd[dialect][len(ground_truth)].append(1.0)\n",
    "\n",
    "for dialect in dialects:\n",
    "    print(f\"======== {dialect} ========\")\n",
    "    print(f'CER  : {np.mean([np.mean(v) for v in editd[dialect].values() if v])*100:.2f}%')\n",
    "    print(f'ACC1 : {np.mean([np.mean(v) for v in acc1[dialect].values() if v])*100:.2f}%')\n",
    "    print(f'ACC20: {np.mean([np.mean(v) for v in acc20[dialect].values() if v])*100:.2f}%')\n",
    "\n",
    "print(f\"======== TOTAL ========\")\n",
    "cert = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in editd.items():\n",
    "    for kk, v in vs.items():\n",
    "        cert[kk].extend(v)\n",
    "acc1t = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in acc1.items():\n",
    "    for kk, v in vs.items():\n",
    "        acc1t[kk].extend(v)\n",
    "\n",
    "acc20t = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in acc20.items():\n",
    "    for kk, v in vs.items():\n",
    "        acc20t[kk].extend(v)\n",
    "print(f'CER : {np.mean([np.mean(v) for v in cert.values() if v])*100:.2f}%')\n",
    "print(f'ACC1 : {np.mean([np.mean(v) for v in acc1t.values() if v])*100:.2f}%')\n",
    "print(f'ACC20 : {np.mean([np.mean(v) for v in acc20t.values() if v])*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AG BERT fine-tuning on the whole iPHI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at pranaydeeps/Ancient-Greek-BERT and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1950' max='977' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [977/977 02:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result: {'eval_loss': 10.620267868041992, 'eval_runtime': 22.2925, 'eval_samples_per_second': 350.297, 'eval_steps_per_second': 43.826}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20492' max='196900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 20492/196900 1:26:38 < 12:25:59, 3.94 it/s, Epoch 10.41/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.695900</td>\n",
       "      <td>2.734945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.632400</td>\n",
       "      <td>2.362947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.371700</td>\n",
       "      <td>2.157843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.243500</td>\n",
       "      <td>2.045613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.133000</td>\n",
       "      <td>1.974687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.056400</td>\n",
       "      <td>1.917562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.976900</td>\n",
       "      <td>1.859798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.945200</td>\n",
       "      <td>1.836522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.871700</td>\n",
       "      <td>1.781387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.870100</td>\n",
       "      <td>1.747057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.838000</td>\n",
       "      <td>1.727401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.787500</td>\n",
       "      <td>1.719864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.766400</td>\n",
       "      <td>1.683851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.748700</td>\n",
       "      <td>1.674828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.744300</td>\n",
       "      <td>1.657410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.712000</td>\n",
       "      <td>1.639457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.686500</td>\n",
       "      <td>1.630785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.672200</td>\n",
       "      <td>1.604801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.662700</td>\n",
       "      <td>1.627898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.625600</td>\n",
       "      <td>1.606030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.606400</td>\n",
       "      <td>1.589221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.613400</td>\n",
       "      <td>1.567315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.616800</td>\n",
       "      <td>1.572814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.598700</td>\n",
       "      <td>1.549318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.567000</td>\n",
       "      <td>1.549242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.583700</td>\n",
       "      <td>1.514711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.527400</td>\n",
       "      <td>1.518402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.545700</td>\n",
       "      <td>1.516925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.530600</td>\n",
       "      <td>1.504104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.535700</td>\n",
       "      <td>1.514832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.503100</td>\n",
       "      <td>1.481996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.495300</td>\n",
       "      <td>1.478570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.493700</td>\n",
       "      <td>1.468405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.475100</td>\n",
       "      <td>1.487075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.492000</td>\n",
       "      <td>1.472226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.466100</td>\n",
       "      <td>1.447254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.446100</td>\n",
       "      <td>1.470405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.458000</td>\n",
       "      <td>1.447725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.452300</td>\n",
       "      <td>1.442622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.438900</td>\n",
       "      <td>1.438747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 67\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation result:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_result)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mEvaluation result: {'eval_loss': 11.396125793457031, 'eval_runtime': 22.5231, 'eval_samples_per_second': 346.8, 'eval_steps_per_second': 14.474}\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/epi-agBERT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/epi-agBERT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/transformers/trainer.py:2744\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2742\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2743\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/accelerate/accelerator.py:1966\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1966\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "#model_name = \"bert-base-uncased\"\n",
    "model_name = \"pranaydeeps/Ancient-Greek-BERT\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name, config=config)\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0)\n",
    "\n",
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data/iphi/train.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    #tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/epi-agBERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=500,\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy = 'steps',\n",
    "    metric_for_best_model= 'eval_loss',\n",
    "    load_best_model_at_end = True,\n",
    "    greater_is_better=False,\n",
    "    logging_dir='./logs',\n",
    "    report_to=[\"tensorboard\"]\n",
    ")\n",
    "\n",
    "val_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data/iphi/validation.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "test_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data/iphi/test.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "test_result = trainer.evaluate(test_dataset)\n",
    "print(\"Evaluation result:\", test_result)\n",
    "\"\"\"\n",
    "Evaluation result: {'eval_loss': 11.396125793457031, 'eval_runtime': 22.5231, 'eval_samples_per_second': 346.8, 'eval_steps_per_second': 14.474}\n",
    "\"\"\"\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model('models/epi-agBERT')\n",
    "tokenizer.save_pretrained('models/epi-agBERT')\n",
    "\n",
    "test_result = trainer.evaluate(test_dataset)\n",
    "print(\"Evaluation result:\", test_result)\n",
    "\n",
    "# with open('log.txt', 'w') as f:\n",
    "#     f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at pranaydeeps/Ancient-Greek-BERT and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 02:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result: {'eval_loss': 10.873405456542969, 'eval_runtime': 0.3262, 'eval_samples_per_second': 291.234, 'eval_steps_per_second': 18.394}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='120000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3000/120000 14:00 < 9:06:25, 3.57 it/s, Epoch 250/10000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.702500</td>\n",
       "      <td>2.649799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.206400</td>\n",
       "      <td>2.222620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.659800</td>\n",
       "      <td>2.104249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.398500</td>\n",
       "      <td>2.284696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.278400</td>\n",
       "      <td>2.488966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.210600</td>\n",
       "      <td>2.338235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n",
      "/data01/alocaputo/miniconda3/envs/ancientGPT/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result: {'eval_loss': 2.165879249572754, 'eval_runtime': 0.3262, 'eval_samples_per_second': 291.2, 'eval_steps_per_second': 18.392, 'epoch': 250.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "import os\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "#model_name = \"bert-base-uncased\"\n",
    "model_name = \"pranaydeeps/Ancient-Greek-BERT\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name, config=config)\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0)\n",
    "\n",
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data/archaic/train.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    #tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/agBERT-archaic\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10000,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=500,\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy = 'steps',\n",
    "    metric_for_best_model= 'eval_loss',\n",
    "    load_best_model_at_end = True,\n",
    "    greater_is_better=False,\n",
    "    logging_dir='./logs/archaic',\n",
    "    run_name=\"agBERT-archaic\",\n",
    ")\n",
    "\n",
    "val_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data/archaic/validation.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "test_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data/archaic/test.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "test_result = trainer.evaluate(test_dataset)\n",
    "print(\"Evaluation result:\", test_result)\n",
    "\"\"\"\n",
    "Evaluation result: {'eval_loss': 11.396125793457031, 'eval_runtime': 22.5231, 'eval_samples_per_second': 346.8, 'eval_steps_per_second': 14.474}\n",
    "\"\"\"\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model('models/agBERT-archaic')\n",
    "tokenizer.save_pretrained('models/agBERT-archaic')\n",
    "\n",
    "test_result = trainer.evaluate(test_dataset)\n",
    "print(\"Evaluation result:\", test_result)\n",
    "\n",
    "# with open('log.txt', 'w') as f:\n",
    "#     f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuned Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without knowing the length of the gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:05<00:00, 16.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== atticionic ========\n",
      "CER  : 88.27%\n",
      "ACC1 : 12.25%\n",
      "ACC20: 28.28%\n",
      "======== doric ========\n",
      "CER  : 80.49%\n",
      "ACC1 : 10.00%\n",
      "ACC20: 27.50%\n",
      "======== northwest ========\n",
      "CER  : 90.83%\n",
      "ACC1 : 20.00%\n",
      "ACC20: 30.00%\n",
      "======== aeolic ========\n",
      "CER  : 80.83%\n",
      "ACC1 : 0.00%\n",
      "ACC20: 0.00%\n",
      "======== TOTAL ========\n",
      "CER : 87.73%\n",
      "ACC1 : 9.95%\n",
      "ACC20 : 23.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"./models/epi-agBERT\"\n",
    "fill_mask = pipeline(\"fill-mask\", model=model_name, tokenizer=model_name, top_k=20)\n",
    "\n",
    "\n",
    "editd = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "acc1 = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "acc20 = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "\n",
    "for inscription in tqdm(test_set):\n",
    "    ground_truth = inscription['masked_gt']\n",
    "    masked = inscription['masked_ag']\n",
    "\n",
    "    dialect = inscription['dialect']\n",
    "    \n",
    "    inference_result = fill_mask(masked)\n",
    "\n",
    "    ctop1 = 0\n",
    "    ctop20 = 0\n",
    "    if ground_truth in [ x['token_str'] for x in inference_result[:1] ]:\n",
    "        ctop1 = 1\n",
    "        ctop20 = 1\n",
    "    if ground_truth in [ x['token_str'] for x in inference_result ]:\n",
    "        ctop20 = 1\n",
    "\n",
    "    acc1[dialect][len(ground_truth)].append(ctop1)\n",
    "    acc20[dialect][len(ground_truth)].append(ctop20)\n",
    "\n",
    "    #editd[dialect][len(ground_truth)].append(editdistance.eval(inference_result[0]['token_str'], ground_truth) / max(len(inference_result[0]['token_str']), len(ground_truth)))\n",
    "    editd[dialect][len(ground_truth)].append(editdistance.eval(inference_result[0]['token_str'], ground_truth) / len(ground_truth))\n",
    "\n",
    "for dialect in dialects:\n",
    "    print(f\"======== {dialect} ========\")\n",
    "    print(f'CER  : {np.mean([np.mean(v) for v in editd[dialect].values() if v])*100:.2f}%')\n",
    "    print(f'ACC1 : {np.mean([np.mean(v) for v in acc1[dialect].values() if v])*100:.2f}%')\n",
    "    print(f'ACC20: {np.mean([np.mean(v) for v in acc20[dialect].values() if v])*100:.2f}%')\n",
    "\n",
    "print(f\"======== TOTAL ========\")\n",
    "cert = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in editd.items():\n",
    "    for kk, v in vs.items():\n",
    "        cert[kk].extend(v)\n",
    "acc1t = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in acc1.items():\n",
    "    for kk, v in vs.items():\n",
    "        acc1t[kk].extend(v)\n",
    "acc20t = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in acc20.items():\n",
    "    for kk, v in vs.items():\n",
    "        acc20t[kk].extend(v)\n",
    "print(f'CER : {np.mean([np.mean(v) for v in cert.values() if v])*100:.2f}%')\n",
    "print(f'ACC1 : {np.mean([np.mean(v) for v in acc1t.values() if v])*100:.2f}%')\n",
    "print(f'ACC20 : {np.mean([np.mean(v) for v in acc20t.values() if v])*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the length of the gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:33<00:00,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== atticionic ========\n",
      "CER  : 68.71%\n",
      "ACC1 : 21.23%\n",
      "ACC20: 28.28%\n",
      "======== doric ========\n",
      "CER  : 73.82%\n",
      "ACC1 : 10.00%\n",
      "ACC20: 27.50%\n",
      "======== northwest ========\n",
      "CER  : 65.00%\n",
      "ACC1 : 30.00%\n",
      "ACC20: 50.00%\n",
      "======== aeolic ========\n",
      "CER  : 89.17%\n",
      "ACC1 : 0.00%\n",
      "ACC20: 0.00%\n",
      "======== TOTAL ========\n",
      "CER : 73.11%\n",
      "ACC1 : 15.82%\n",
      "ACC20 : 25.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"./models/epi-agBERT\"\n",
    "fill_mask = pipeline(\"fill-mask\", model=model_name, tokenizer=model_name, top_k=3500)\n",
    "\n",
    "ctop1s = []\n",
    "ctop20s = []\n",
    "\n",
    "editd = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "\n",
    "acc1 = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "acc20 = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "\n",
    "pbar = tqdm(test_set)\n",
    "for inscription in pbar:\n",
    "    ground_truth = inscription['masked_gt']\n",
    "    masked = inscription['masked_ag']\n",
    "\n",
    "    dialect = inscription['dialect']\n",
    "    \n",
    "    inference_result = fill_mask(masked)\n",
    "\n",
    "    constr = [ x for x in inference_result if len(x['token_str']) == len(ground_truth)][:20]\n",
    "\n",
    "    ctop1 = 0\n",
    "    ctop20 = 0\n",
    "    if ground_truth in [ x['token_str'] for x in constr[:1] ]:\n",
    "        ctop1 = 1\n",
    "        ctop20 = 1\n",
    "    if ground_truth in [ x['token_str'] for x in constr ]:\n",
    "        ctop20 = 1\n",
    "\n",
    "    ctop1s.append(ctop1)\n",
    "    ctop20s.append(ctop20)\n",
    "    acc1[dialect][len(ground_truth)].append(ctop1)\n",
    "    acc20[dialect][len(ground_truth)].append(ctop20)\n",
    "\n",
    "    if constr != []:\n",
    "        editd[dialect][len(ground_truth)].append(editdistance.eval(constr[0]['token_str'], ground_truth) / len(ground_truth))\n",
    "    else:\n",
    "        editd[dialect][len(ground_truth)].append(1.0)\n",
    "\n",
    "for dialect in dialects:\n",
    "    print(f\"======== {dialect} ========\")\n",
    "    print(f'CER  : {np.mean([np.mean(v) for v in editd[dialect].values() if v])*100:.2f}%')\n",
    "    print(f'ACC1 : {np.mean([np.mean(v) for v in acc1[dialect].values() if v])*100:.2f}%')\n",
    "    print(f'ACC20: {np.mean([np.mean(v) for v in acc20[dialect].values() if v])*100:.2f}%')\n",
    "\n",
    "print(f\"======== TOTAL ========\")\n",
    "cert = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in editd.items():\n",
    "    for kk, v in vs.items():\n",
    "        cert[kk].extend(v)\n",
    "acc1t = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in acc1.items():\n",
    "    for kk, v in vs.items():\n",
    "        acc1t[kk].extend(v)\n",
    "\n",
    "acc20t = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in acc20.items():\n",
    "    for kk, v in vs.items():\n",
    "        acc20t[kk].extend(v)\n",
    "print(f'CER : {np.mean([np.mean(v) for v in cert.values() if v])*100:.2f}%')\n",
    "print(f'ACC1 : {np.mean([np.mean(v) for v in acc1t.values() if v])*100:.2f}%')\n",
    "print(f'ACC20 : {np.mean([np.mean(v) for v in acc20t.values() if v])*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archaic knowing the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:33<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== atticionic ========\n",
      "CER  : 65.88%\n",
      "ACC1 : 26.36%\n",
      "ACC20: 28.28%\n",
      "======== doric ========\n",
      "CER  : 64.25%\n",
      "ACC1 : 27.50%\n",
      "ACC20: 27.50%\n",
      "======== northwest ========\n",
      "CER  : 56.67%\n",
      "ACC1 : 40.00%\n",
      "ACC20: 50.00%\n",
      "======== aeolic ========\n",
      "CER  : 80.83%\n",
      "ACC1 : 0.00%\n",
      "ACC20: 0.00%\n",
      "======== TOTAL ========\n",
      "CER : 69.01%\n",
      "ACC1 : 22.81%\n",
      "ACC20 : 25.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"./models/agBERT-archaic/\" #\"./models/agBERT-iphi-archaic/\"\n",
    "fill_mask = pipeline(\"fill-mask\", model=model_name, tokenizer=model_name, top_k=3500)\n",
    "\n",
    "ctop1s = []\n",
    "ctop20s = []\n",
    "\n",
    "dialects = ['atticionic', 'doric', 'northwest', 'aeolic']\n",
    "\n",
    "editd = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "acc1 = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "acc20 = { d: { x: [] for x in range(1, max_l+1)} for d in dialects}\n",
    "\n",
    "for inscription in tqdm(test_set):\n",
    "    ground_truth = inscription['masked_gt']\n",
    "    masked = inscription['masked_ag']\n",
    "\n",
    "    dialect = inscription['dialect']\n",
    "    \n",
    "    inference_result = fill_mask(masked)\n",
    "\n",
    "    constr = [ x for x in inference_result if len(x['token_str']) == len(ground_truth)][:20]\n",
    "\n",
    "    ctop1 = 0\n",
    "    ctop20 = 0\n",
    "    if ground_truth in [ x['token_str'] for x in constr[:1] ]:\n",
    "        ctop1 = 1\n",
    "        ctop20 = 1\n",
    "    if ground_truth in [ x['token_str'] for x in constr ]:\n",
    "        ctop20 = 1\n",
    "\n",
    "    ctop1s.append(ctop1)\n",
    "    ctop20s.append(ctop20)\n",
    "    acc1[dialect][len(ground_truth)].append(ctop1)\n",
    "    acc20[dialect][len(ground_truth)].append(ctop20)\n",
    "\n",
    "    if constr != []:\n",
    "        editd[dialect][len(ground_truth)].append(editdistance.eval(constr[0]['token_str'], ground_truth) / len(ground_truth))\n",
    "    else:\n",
    "        editd[dialect][len(ground_truth)].append(1.0)\n",
    "    #pbar.set_description(f\"acc1={ctop1s.count(1)/len(ctop1s)*100:.2f}%, acc20={ctop20s.count(1)/len(ctop20s)*100:.2f}%\")\n",
    "\n",
    "for dialect in dialects:\n",
    "    print(f\"======== {dialect} ========\")\n",
    "    print(f'CER  : {np.mean([np.mean(v) for v in editd[dialect].values() if v])*100:.2f}%')\n",
    "    print(f'ACC1 : {np.mean([np.mean(v) for v in acc1[dialect].values() if v])*100:.2f}%')\n",
    "    print(f'ACC20: {np.mean([np.mean(v) for v in acc20[dialect].values() if v])*100:.2f}%')\n",
    "    \n",
    "print(f\"======== TOTAL ========\")\n",
    "acc1t = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in acc1.items():\n",
    "    for kk, v in vs.items():\n",
    "        acc1t[kk].extend(v)\n",
    "\n",
    "acc20t = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in acc20.items():\n",
    "    for kk, v in vs.items():\n",
    "        acc20t[kk].extend(v)\n",
    "\n",
    "cert = { x: [] for x in range(1, max_l+1)}\n",
    "for k, vs in editd.items():\n",
    "    for kk, v in vs.items():\n",
    "        cert[kk].extend(v)\n",
    "        \n",
    "print(f'CER : {np.mean([np.mean(v) for v in cert.values() if v])*100:.2f}%')\n",
    "print(f'ACC1 : {np.mean([np.mean(v) for v in acc1t.values() if v])*100:.2f}%')\n",
    "print(f'ACC20 : {np.mean([np.mean(v) for v in acc20t.values() if v])*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ancientGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
